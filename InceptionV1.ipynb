{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/mariomamdouh23/finetuning-inception-v1-with-cifar10-dataset.c414c925-6cd0-4481-b0f1-988e8de5e0cd.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20251211/auto/storage/goog4_request&X-Goog-Date=20251211T143525Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=014b3472da204799a86af6ad8ffb4dc1e5a2965e285bdb11164ba40ace44d4efa67fda53c19978c180469d3965b8e41199ba10658c06fa53eda0ef7e3faed5efda38763951be6023d304d8a2d5208b807685dddf3ddfc74ba124d5b85d697276e343f4ffac44906a5d48f890e8dde8dc04dc1318bbc925ac14126c666895ff008c8d740cb325b22d47cef22fe5703521c58da227b46be7bbea128f53171f5bd3250e34baa5e4801e87df150ee1727e7858ca87aff3e8b08931fc52cf51cf905a180197d0d3598ae742728f37f53cd591fe54ca5a1dc4d962bab5b80658c42fdf79c452db17d0e5735758bd4a02b7409b1c71fe0cf5df35f2a46e0688751cb8e1","timestamp":1765491380324}],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.utils.data import random_split\n\n\nclass CarsFolderDataset(Dataset):\n    \"\"\"\n    Dataset for folder-based structure where each folder is a class.\n    \n    Expected structure:\n        root_dir/\n            car1/\n                image1.jpg\n                image2.jpg\n                ...\n            car2/\n                image1.jpg\n                image2.jpg\n                ...\n    \"\"\"\n    def __init__(self, root_dir, transform=None, num_classes=20):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.num_classes = num_classes\n        \n        self.files = []\n        self.labels = []\n        \n        # Get all class folders (sorted for consistency)\n        all_class_folders = sorted([d for d in os.listdir(root_dir) \n                                    if os.path.isdir(os.path.join(root_dir, d))])\n        \n        # Take only first num_classes\n        class_folders = all_class_folders[:num_classes]\n        \n        # Create class to index mapping\n        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_folders)}\n        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()}\n        \n        # Load all images\n        for class_name in class_folders:\n            class_path = os.path.join(root_dir, class_name)\n            class_idx = self.class_to_idx[class_name]\n            \n            # Get all image files in this class folder\n            for img_name in os.listdir(class_path):\n                img_path = os.path.join(class_path, img_name)\n                # Check if it's a file (not a subdirectory)\n                if os.path.isfile(img_path):\n                    # Optional: filter by image extensions\n                    if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n                        self.files.append(img_path)\n                        self.labels.append(class_idx)\n        \n        print(f\"Loaded {len(self.files)} images from {len(class_folders)} classes.\")\n        print(f\"Classes: {class_folders}\")\n    \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        img_path = self.files[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        label = self.labels[idx]\n        return img, label","metadata":{"id":"eFL0W_nwOrDq","executionInfo":{"status":"ok","timestamp":1765489679808,"user_tz":-120,"elapsed":3755,"user":{"displayName":"","userId":""}},"outputId":"adc235b2-c1d7-41c3-82ba-f8268edec490","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:14:48.146791Z","iopub.execute_input":"2025-12-15T14:14:48.147140Z","iopub.status.idle":"2025-12-15T14:15:00.642870Z","shell.execute_reply.started":"2025-12-15T14:14:48.147110Z","shell.execute_reply":"2025-12-15T14:15:00.641249Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class AugmentedDataset(Dataset):\n    \"\"\"\n    Dataset wrapper that creates and saves augmented versions of images.\n    Expands dataset by 4x: original + 3 augmentations per image.\n    \"\"\"\n    def __init__(self, base_dataset, save_dir, augment_transform, transform=None):\n        self.base = base_dataset\n        self.save_dir = save_dir\n        self.transform = transform\n        self.augment_transform = augment_transform\n        os.makedirs(self.save_dir, exist_ok=True)\n        self.aug_paths = []\n        self._prepare_augmented_images()\n    \n    def _prepare_augmented_images(self):\n        print(\"Preparing augmented images...\")\n        for idx in range(len(self.base)):\n            # Use the full file path and sanitize it for filename\n            fname = self.base.files[idx].replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n            orig_path = os.path.join(self.save_dir, f\"{fname}_orig.jpg\")\n            aug_paths = [\n                os.path.join(self.save_dir, f\"{fname}_aug1.jpg\"),\n                os.path.join(self.save_dir, f\"{fname}_aug2.jpg\"),\n                os.path.join(self.save_dir, f\"{fname}_aug3.jpg\"),\n            ]\n            \n            # Only create if missing\n            if not all(os.path.exists(p) for p in aug_paths):\n                # Load original image\n                img = Image.open(self.base.files[idx]).convert(\"RGB\")\n                \n                # Save original\n                if not os.path.exists(orig_path):\n                    img.save(orig_path)\n                \n                # Create and save 3 augmentations\n                for p in aug_paths:\n                    aug_img = self.augment_transform(img)\n                    aug_img.save(p)\n            \n            self.aug_paths.append([orig_path] + aug_paths)\n        \n        print(f\"Augmented dataset ready: {len(self.aug_paths)} base images × 4 = {len(self)} total samples\")\n    \n    def __len__(self):\n        return len(self.base) * 4  # original + 3 augmentations\n    \n    def __getitem__(self, idx):\n        base_idx = idx // 4\n        aug_idx = idx % 4  # select which augmentation (0=orig, 1-3=augs)\n        img_path = self.aug_paths[base_idx][aug_idx]\n        label = self.base.labels[base_idx]\n        \n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label","metadata":{"id":"UAxjQsSHOHyf","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:15:00.644575Z","iopub.execute_input":"2025-12-15T14:15:00.645035Z","iopub.status.idle":"2025-12-15T14:15:00.658188Z","shell.execute_reply.started":"2025-12-15T14:15:00.645008Z","shell.execute_reply":"2025-12-15T14:15:00.656474Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"augment_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=1.0),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n])\n    \n# Training transform (applied when loading)\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])","metadata":{"id":"XbyjFB03OwsW","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:15:00.659932Z","iopub.execute_input":"2025-12-15T14:15:00.660315Z","iopub.status.idle":"2025-12-15T14:15:00.696125Z","shell.execute_reply.started":"2025-12-15T14:15:00.660290Z","shell.execute_reply":"2025-12-15T14:15:00.693935Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"base_train_dataset = CarsFolderDataset(\n    root_dir='/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/train',\n    transform=None  # No transform for base dataset\n)\n\naug_dataset = AugmentedDataset(\n    base_dataset=base_train_dataset,\n    save_dir='augmented_train',\n    augment_transform=augment_transform,\n    transform=train_transform\n)\n# Test dataset WITHOUT augmentation\ntest_dataset = CarsFolderDataset(\n    root_dir='/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/test',\n    transform=test_transform\n)\n\n\n# 80/20 split\ntrain_size = int(0.8 * len(aug_dataset))\nval_size = len(aug_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(aug_dataset, [train_size, val_size])","metadata":{"id":"jG1VG-2YPbhh","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:15:00.699084Z","iopub.execute_input":"2025-12-15T14:15:00.699470Z","iopub.status.idle":"2025-12-15T14:16:11.236252Z","shell.execute_reply.started":"2025-12-15T14:15:00.699444Z","shell.execute_reply":"2025-12-15T14:16:11.234870Z"}},"outputs":[{"name":"stdout","text":"Loaded 819 images from 20 classes.\nClasses: ['AM General Hummer SUV 2000', 'Acura Integra Type R 2001', 'Acura RL Sedan 2012', 'Acura TL Sedan 2012', 'Acura TL Type-S 2008', 'Acura TSX Sedan 2012', 'Acura ZDX Hatchback 2012', 'Aston Martin V8 Vantage Convertible 2012', 'Aston Martin V8 Vantage Coupe 2012', 'Aston Martin Virage Convertible 2012', 'Aston Martin Virage Coupe 2012', 'Audi 100 Sedan 1994', 'Audi 100 Wagon 1994', 'Audi A5 Coupe 2012', 'Audi R8 Coupe 2012', 'Audi RS 4 Convertible 2008', 'Audi S4 Sedan 2007', 'Audi S4 Sedan 2012', 'Audi S5 Convertible 2012', 'Audi S5 Coupe 2012']\nPreparing augmented images...\nAugmented dataset ready: 819 base images × 4 = 3276 total samples\nLoaded 811 images from 20 classes.\nClasses: ['AM General Hummer SUV 2000', 'Acura Integra Type R 2001', 'Acura RL Sedan 2012', 'Acura TL Sedan 2012', 'Acura TL Type-S 2008', 'Acura TSX Sedan 2012', 'Acura ZDX Hatchback 2012', 'Aston Martin V8 Vantage Convertible 2012', 'Aston Martin V8 Vantage Coupe 2012', 'Aston Martin Virage Convertible 2012', 'Aston Martin Virage Coupe 2012', 'Audi 100 Sedan 1994', 'Audi 100 Wagon 1994', 'Audi A5 Coupe 2012', 'Audi R8 Coupe 2012', 'Audi RS 4 Convertible 2008', 'Audi S4 Sedan 2007', 'Audi S4 Sedan 2012', 'Audi S5 Convertible 2012', 'Audi S5 Coupe 2012']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"id":"Un3LsTFDXuie","executionInfo":{"status":"ok","timestamp":1765489764435,"user_tz":-120,"elapsed":19,"user":{"displayName":"","userId":""}},"outputId":"d7ca9e6c-100f-4b82-ba56-0e92d8294119","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:16:11.237260Z","iopub.execute_input":"2025-12-15T14:16:11.237525Z","iopub.status.idle":"2025-12-15T14:16:11.247380Z","shell.execute_reply.started":"2025-12-15T14:16:11.237504Z","shell.execute_reply":"2025-12-15T14:16:11.246057Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)","metadata":{"id":"I8_5vjgUUSfw","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:16:11.248569Z","iopub.execute_input":"2025-12-15T14:16:11.248941Z","iopub.status.idle":"2025-12-15T14:16:11.272655Z","shell.execute_reply.started":"2025-12-15T14:16:11.248909Z","shell.execute_reply":"2025-12-15T14:16:11.271562Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from torch import nn\nimport torchvision.models as models\nfrom torch.optim import Adam\nfrom torch.nn import Identity\nfrom torch.nn import CrossEntropyLoss\n\nclass Inception(nn.Module):\n    def __init__(self, num_output) -> None:\n        super().__init__()\n        # Load pretrained GoogLeNet (Inception v1)\n        inception = models.googlenet(weights=models.GoogLeNet_Weights.DEFAULT)\n\n        # Freeze all pretrained layers\n        # for param in inception.parameters():\n        #     param.requires_grad = False\n\n        # Replace the final fully-connected layer\n        num_filters = inception.fc.in_features\n        inception.fc = nn.Linear(num_filters, num_output)\n\n        self.inception_v1 = inception\n\n    def forward(self, x):\n        return self.inception_v1(x)","metadata":{"id":"kdZzdatlO8pj","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:18:29.946143Z","iopub.execute_input":"2025-12-15T14:18:29.947167Z","iopub.status.idle":"2025-12-15T14:18:29.953712Z","shell.execute_reply.started":"2025-12-15T14:18:29.947126Z","shell.execute_reply":"2025-12-15T14:18:29.952735Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model = Inception(20).to(device)","metadata":{"id":"fH3wnqHVSFrK","executionInfo":{"status":"ok","timestamp":1765489768761,"user_tz":-120,"elapsed":711,"user":{"displayName":"","userId":""}},"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:18:30.294229Z","iopub.execute_input":"2025-12-15T14:18:30.294536Z","iopub.status.idle":"2025-12-15T14:18:30.482869Z","shell.execute_reply.started":"2025-12-15T14:18:30.294514Z","shell.execute_reply":"2025-12-15T14:18:30.481752Z"},"outputId":"171be6b3-6b07-44d1-f830-2c211333df78","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"EPOCHS = 10\nlearning_rate = 1e-3\ncriterion = CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"id":"YUd6C41VXKwu","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:18:32.933167Z","iopub.execute_input":"2025-12-15T14:18:32.934197Z","iopub.status.idle":"2025-12-15T14:18:32.940421Z","shell.execute_reply.started":"2025-12-15T14:18:32.934152Z","shell.execute_reply":"2025-12-15T14:18:32.939443Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import datetime as dt\nfrom tqdm import tqdm\n\ntotal_loss_train_plot = []\ntotal_acc_train_plot = []\ntotal_loss_test_plot = []\ntotal_acc_test_plot = []\n\nstart = dt.datetime.now()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_train_loss = 0\n    correct_train = 0\n    total_train = 0\n\n    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n    train_loop = tqdm(train_loader, desc=\"Training\", leave=False)\n\n    for inputs, labels in train_loop:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        total_train_loss += loss.item()\n\n        # --- Accuracy ---\n        preds = torch.argmax(outputs, dim=1)\n        correct_train += (preds == labels).sum().item()\n        total_train += labels.size(0)\n\n        loss.backward()\n        optimizer.step()\n\n        train_loop.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n\n    # --- Validation ---\n    model.eval()\n    total_val_loss = 0\n    correct_val = 0\n    total_val = 0\n\n    val_loop = tqdm(val_loader, desc=\"Validating\", leave=False)\n    with torch.no_grad():\n        for inputs, labels in val_loop:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_val_loss += loss.item()\n\n            # --- Accuracy ---\n            preds = torch.argmax(outputs, dim=1)\n            correct_val += (preds == labels).sum().item()\n            total_val += labels.size(0)\n\n            val_loop.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n\n    # --- Compute averages ---\n    avg_train_loss = total_train_loss / len(train_loader)\n    avg_val_loss = total_val_loss / len(val_loader)\n    train_acc = correct_train / total_train\n    val_acc = correct_val / total_val\n\n    total_loss_train_plot.append(avg_train_loss)\n    total_acc_train_plot.append(train_acc)\n    total_loss_test_plot.append(avg_val_loss)\n    total_acc_test_plot.append(val_acc)\n\n    print(f\"Train Loss (MAE): {avg_train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n    print(f\"Val Loss (MAE):   {avg_val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n    print(\"=\" * 60)\n\nend = dt.datetime.now()\nprint(f\"Training completed in: {end - start}\")","metadata":{"id":"pvn0WDyUXpIO","executionInfo":{"status":"ok","timestamp":1765490601549,"user_tz":-120,"elapsed":52630,"user":{"displayName":"","userId":""}},"outputId":"5d343155-59da-44bf-ca59-dfe2ca0715dd","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:18:36.439080Z","iopub.execute_input":"2025-12-15T14:18:36.439372Z","iopub.status.idle":"2025-12-15T15:35:44.870861Z","shell.execute_reply.started":"2025-12-15T14:18:36.439350Z","shell.execute_reply":"2025-12-15T15:35:44.869746Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 1.5655 | Train Acc: 53.70%\nVal Loss (MAE):   1.0467 | Val Acc:   63.11%\n============================================================\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.3261 | Train Acc: 91.15%\nVal Loss (MAE):   1.0292 | Val Acc:   66.77%\n============================================================\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.1003 | Train Acc: 97.71%\nVal Loss (MAE):   0.4702 | Val Acc:   83.23%\n============================================================\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.0698 | Train Acc: 98.05%\nVal Loss (MAE):   0.3699 | Val Acc:   87.96%\n============================================================\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.0458 | Train Acc: 98.93%\nVal Loss (MAE):   0.3416 | Val Acc:   90.40%\n============================================================\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.0615 | Train Acc: 98.44%\nVal Loss (MAE):   0.5694 | Val Acc:   84.30%\n============================================================\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.1141 | Train Acc: 96.72%\nVal Loss (MAE):   1.0381 | Val Acc:   69.05%\n============================================================\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.1394 | Train Acc: 95.92%\nVal Loss (MAE):   0.5672 | Val Acc:   83.69%\n============================================================\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.1080 | Train Acc: 97.10%\nVal Loss (MAE):   0.9873 | Val Acc:   71.80%\n============================================================\n\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"                                                                            ","output_type":"stream"},{"name":"stdout","text":"Train Loss (MAE): 0.0747 | Train Acc: 97.90%\nVal Loss (MAE):   0.6738 | Val Acc:   82.62%\n============================================================\nTraining completed in: 1:17:08.419771\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns\nimport os\n\nos.makedirs(\"/kaggle/working/metrics\", exist_ok=True)\n\nmodel.eval()\nall_labels = []\nall_preds = []\nall_probs = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        probs = torch.softmax(outputs, dim=1)\n        preds = outputs.argmax(dim=1)\n\n        all_labels.append(labels.cpu().numpy())\n        all_preds.append(preds.cpu().numpy())\n        all_probs.append(probs.cpu().numpy())\n\nall_labels = np.concatenate(all_labels)\nall_preds = np.concatenate(all_preds)\nall_probs = np.concatenate(all_probs)\n\n# ---- Confusion Matrix as PNG ----\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(10,8))\nsns.heatmap(cm, annot=False, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.savefig(\"/kaggle/working/metrics/Inception_confusion_matrix.png\")\nplt.close()\n\n# ---- Classification Report as TXT ----\nreport = classification_report(all_labels, all_preds, zero_division=0)\nwith open(\"/kaggle/working/metrics/Inception_classification_report.txt\", \"w\") as f:\n    f.write(report)\n\nprint(\"Classification Report:\\n\", report)\n\n# ---- Smart ROC & AUC ----\nMAX_ROC_SAMPLES = 1500  # optional to speed up\n\nif len(all_labels) > MAX_ROC_SAMPLES:\n    idx = np.random.choice(len(all_labels), MAX_ROC_SAMPLES, replace=False)\n    y_true = all_labels[idx]\n    y_prob = all_probs[idx]\nelse:\n    y_true = all_labels\n    y_prob = all_probs\n\nNUM_CLASSES = y_prob.shape[1]\ny_true_bin = label_binarize(y_true, classes=list(range(NUM_CLASSES)))\n\n# Micro-average ROC\nfpr_micro, tpr_micro, _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\nroc_auc_micro = auc(fpr_micro, tpr_micro)\n\n# Macro-average ROC\nfpr_macro = np.linspace(0, 1, 100)\ntpr_macro = np.zeros_like(fpr_macro)\nfor i in range(NUM_CLASSES):\n    fpr_i, tpr_i, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n    tpr_macro += np.interp(fpr_macro, fpr_i, tpr_i)\ntpr_macro /= NUM_CLASSES\nroc_auc_macro = auc(fpr_macro, tpr_macro)\n\n# Save ROC AUC in TXT\nwith open(\"/kaggle/working/metrics/Inception_roc_auc.txt\", \"w\") as f:\n    f.write(f\"Micro-average AUC: {roc_auc_micro:.4f}\\n\")\n    f.write(f\"Macro-average AUC: {roc_auc_macro:.4f}\\n\")\n\n# Optional: Save ROC curves as image\nplt.figure(figsize=(8,6))\nplt.plot(fpr_micro, tpr_micro,\n         label=f'Micro-average ROC (AUC = {roc_auc_micro:.4f})', linewidth=2)\nplt.plot(fpr_macro, tpr_macro,\n         label=f'Macro-average ROC (AUC = {roc_auc_macro:.4f})', linewidth=2)\nplt.plot([0,1], [0,1], 'k--', linewidth=1)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves')\nplt.legend(loc='lower right')\nplt.savefig(\"/kaggle/working/metrics/Inception_roc_curves.png\")\nplt.close()\n\nprint(f\"Micro AUC: {roc_auc_micro:.4f}, Macro AUC: {roc_auc_macro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T15:46:21.151086Z","iopub.execute_input":"2025-12-15T15:46:21.155056Z","iopub.status.idle":"2025-12-15T15:47:53.285247Z","shell.execute_reply.started":"2025-12-15T15:46:21.155013Z","shell.execute_reply":"2025-12-15T15:47:53.284088Z"},"id":"8cVzDHOp1pzx","executionInfo":{"status":"ok","timestamp":1765491059798,"user_tz":-120,"elapsed":612,"user":{"displayName":"","userId":""}},"outputId":"0bf6714c-7db5-4f7f-c26b-a4d8b5fe4c77","colab":{"base_uri":"https://localhost:8080/","height":427}},"outputs":[{"name":"stdout","text":"Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.95      0.95        44\n           1       1.00      0.57      0.72        44\n           2       0.47      0.22      0.30        32\n           3       0.94      0.37      0.53        43\n           4       0.63      0.86      0.73        42\n           5       1.00      0.42      0.60        40\n           6       0.47      0.72      0.57        39\n           7       1.00      0.09      0.16        45\n           8       0.32      0.83      0.47        41\n           9       0.67      0.61      0.63        33\n          10       0.63      0.87      0.73        38\n          11       0.81      0.62      0.70        40\n          12       0.74      0.55      0.63        42\n          13       0.49      0.90      0.63        41\n          14       0.86      0.84      0.85        43\n          15       0.78      0.50      0.61        36\n          16       0.61      0.76      0.67        45\n          17       0.26      0.54      0.35        39\n          18       0.69      0.60      0.64        42\n          19       0.40      0.10      0.15        42\n\n    accuracy                           0.60       811\n   macro avg       0.69      0.60      0.58       811\nweighted avg       0.69      0.60      0.59       811\n\nMicro AUC: 0.9395, Macro AUC: 0.9467\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}