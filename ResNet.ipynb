{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":300657,"sourceType":"datasetVersion","datasetId":124720}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.utils.data import random_split\n\n\nclass CarsFolderDataset(Dataset):\n    \"\"\"\n    Dataset for folder-based structure where each folder is a class.\n    \n    Expected structure:\n        root_dir/\n            car1/\n                image1.jpg\n                image2.jpg\n                ...\n            car2/\n                image1.jpg\n                image2.jpg\n                ...\n    \"\"\"\n    def __init__(self, root_dir, transform=None, num_classes=20):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.num_classes = num_classes\n        \n        self.files = []\n        self.labels = []\n        \n        # Get all class folders (sorted for consistency)\n        all_class_folders = sorted([d for d in os.listdir(root_dir) \n                                    if os.path.isdir(os.path.join(root_dir, d))])\n        \n        # Take only first num_classes\n        class_folders = all_class_folders[:num_classes]\n        \n        # Create class to index mapping\n        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_folders)}\n        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()}\n        \n        # Load all images\n        for class_name in class_folders:\n            class_path = os.path.join(root_dir, class_name)\n            class_idx = self.class_to_idx[class_name]\n            \n            # Get all image files in this class folder\n            for img_name in os.listdir(class_path):\n                img_path = os.path.join(class_path, img_name)\n                # Check if it's a file (not a subdirectory)\n                if os.path.isfile(img_path):\n                    # Optional: filter by image extensions\n                    if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n                        self.files.append(img_path)\n                        self.labels.append(class_idx)\n        \n        print(f\"Loaded {len(self.files)} images from {len(class_folders)} classes.\")\n        print(f\"Classes: {class_folders}\")\n    \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        img_path = self.files[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        label = self.labels[idx]\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:49:36.161671Z","iopub.execute_input":"2025-12-15T13:49:36.162226Z","iopub.status.idle":"2025-12-15T13:49:44.564379Z","shell.execute_reply.started":"2025-12-15T13:49:36.162178Z","shell.execute_reply":"2025-12-15T13:49:44.563760Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class AugmentedDataset(Dataset):\n    \"\"\"\n    Dataset wrapper that creates and saves augmented versions of images.\n    Expands dataset by 4x: original + 3 augmentations per image.\n    \"\"\"\n    def __init__(self, base_dataset, save_dir, augment_transform, transform=None):\n        self.base = base_dataset\n        self.save_dir = save_dir\n        self.transform = transform\n        self.augment_transform = augment_transform\n        os.makedirs(self.save_dir, exist_ok=True)\n        self.aug_paths = []\n        self._prepare_augmented_images()\n    \n    def _prepare_augmented_images(self):\n        print(\"Preparing augmented images...\")\n        for idx in range(len(self.base)):\n            # Use the full file path and sanitize it for filename\n            fname = self.base.files[idx].replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n            orig_path = os.path.join(self.save_dir, f\"{fname}_orig.jpg\")\n            aug_paths = [\n                os.path.join(self.save_dir, f\"{fname}_aug1.jpg\"),\n                os.path.join(self.save_dir, f\"{fname}_aug2.jpg\"),\n                os.path.join(self.save_dir, f\"{fname}_aug3.jpg\"),\n            ]\n            \n            # Only create if missing\n            if not all(os.path.exists(p) for p in aug_paths):\n                # Load original image\n                img = Image.open(self.base.files[idx]).convert(\"RGB\")\n                \n                # Save original\n                if not os.path.exists(orig_path):\n                    img.save(orig_path)\n                \n                # Create and save 3 augmentations\n                for p in aug_paths:\n                    aug_img = self.augment_transform(img)\n                    aug_img.save(p)\n            \n            self.aug_paths.append([orig_path] + aug_paths)\n        \n        print(f\"Augmented dataset ready: {len(self.aug_paths)} base images × 4 = {len(self)} total samples\")\n    \n    def __len__(self):\n        return len(self.base) * 4  # original + 3 augmentations\n    \n    def __getitem__(self, idx):\n        base_idx = idx // 4\n        aug_idx = idx % 4  # select which augmentation (0=orig, 1-3=augs)\n        img_path = self.aug_paths[base_idx][aug_idx]\n        label = self.base.labels[base_idx]\n        \n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:49:44.565525Z","iopub.execute_input":"2025-12-15T13:49:44.565937Z","iopub.status.idle":"2025-12-15T13:49:44.575189Z","shell.execute_reply.started":"2025-12-15T13:49:44.565915Z","shell.execute_reply":"2025-12-15T13:49:44.574422Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"augment_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=1.0),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n])\n    \n# Training transform (applied when loading)\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:49:44.576012Z","iopub.execute_input":"2025-12-15T13:49:44.576356Z","iopub.status.idle":"2025-12-15T13:49:44.616971Z","shell.execute_reply.started":"2025-12-15T13:49:44.576326Z","shell.execute_reply":"2025-12-15T13:49:44.616243Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"base_train_dataset = CarsFolderDataset(\n    root_dir='/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/train',\n    transform=None  # No transform for base dataset\n)\n\naug_dataset = AugmentedDataset(\n    base_dataset=base_train_dataset,\n    save_dir='augmented_train',\n    augment_transform=augment_transform,\n    transform=train_transform\n)\n# Test dataset WITHOUT augmentation\ntest_dataset = CarsFolderDataset(\n    root_dir='/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/test',\n    transform=test_transform\n)\n\n\n# 80/20 split\ntrain_size = int(0.8 * len(aug_dataset))\nval_size = len(aug_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(aug_dataset, [train_size, val_size])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:49:44.618564Z","iopub.execute_input":"2025-12-15T13:49:44.618863Z","iopub.status.idle":"2025-12-15T13:50:45.523374Z","shell.execute_reply.started":"2025-12-15T13:49:44.618844Z","shell.execute_reply":"2025-12-15T13:50:45.522505Z"}},"outputs":[{"name":"stdout","text":"Loaded 819 images from 20 classes.\nClasses: ['AM General Hummer SUV 2000', 'Acura Integra Type R 2001', 'Acura RL Sedan 2012', 'Acura TL Sedan 2012', 'Acura TL Type-S 2008', 'Acura TSX Sedan 2012', 'Acura ZDX Hatchback 2012', 'Aston Martin V8 Vantage Convertible 2012', 'Aston Martin V8 Vantage Coupe 2012', 'Aston Martin Virage Convertible 2012', 'Aston Martin Virage Coupe 2012', 'Audi 100 Sedan 1994', 'Audi 100 Wagon 1994', 'Audi A5 Coupe 2012', 'Audi R8 Coupe 2012', 'Audi RS 4 Convertible 2008', 'Audi S4 Sedan 2007', 'Audi S4 Sedan 2012', 'Audi S5 Convertible 2012', 'Audi S5 Coupe 2012']\nPreparing augmented images...\nAugmented dataset ready: 819 base images × 4 = 3276 total samples\nLoaded 811 images from 20 classes.\nClasses: ['AM General Hummer SUV 2000', 'Acura Integra Type R 2001', 'Acura RL Sedan 2012', 'Acura TL Sedan 2012', 'Acura TL Type-S 2008', 'Acura TSX Sedan 2012', 'Acura ZDX Hatchback 2012', 'Aston Martin V8 Vantage Convertible 2012', 'Aston Martin V8 Vantage Coupe 2012', 'Aston Martin Virage Convertible 2012', 'Aston Martin Virage Coupe 2012', 'Audi 100 Sedan 1994', 'Audi 100 Wagon 1994', 'Audi A5 Coupe 2012', 'Audi R8 Coupe 2012', 'Audi RS 4 Convertible 2008', 'Audi S4 Sedan 2007', 'Audi S4 Sedan 2012', 'Audi S5 Convertible 2012', 'Audi S5 Coupe 2012']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(f\"Dataset size before augmentation: {len(base_train_dataset)}\")\nprint(f\"Dataset size after augmentation: {len(aug_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:50:45.524316Z","iopub.execute_input":"2025-12-15T13:50:45.524539Z","iopub.status.idle":"2025-12-15T13:50:45.528965Z","shell.execute_reply.started":"2025-12-15T13:50:45.524521Z","shell.execute_reply":"2025-12-15T13:50:45.528073Z"}},"outputs":[{"name":"stdout","text":"Dataset size before augmentation: 819\nDataset size after augmentation: 3276\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True\n)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=64,\n    shuffle=False\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=64,\n    shuffle=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:50:45.529904Z","iopub.execute_input":"2025-12-15T13:50:45.530505Z","iopub.status.idle":"2025-12-15T13:50:45.541823Z","shell.execute_reply.started":"2025-12-15T13:50:45.530473Z","shell.execute_reply":"2025-12-15T13:50:45.540859Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import resnet34","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:50:45.542703Z","iopub.execute_input":"2025-12-15T13:50:45.542975Z","iopub.status.idle":"2025-12-15T13:50:45.552689Z","shell.execute_reply.started":"2025-12-15T13:50:45.542951Z","shell.execute_reply":"2025-12-15T13:50:45.552002Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define the model\nmodel = resnet34(pretrained=True)\n\n# Replace the last layer\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:50:45.553480Z","iopub.execute_input":"2025-12-15T13:50:45.553745Z","iopub.status.idle":"2025-12-15T13:50:46.538642Z","shell.execute_reply.started":"2025-12-15T13:50:45.553714Z","shell.execute_reply":"2025-12-15T13:50:46.537968Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 181MB/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Move the model to the device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:50:46.539517Z","iopub.execute_input":"2025-12-15T13:50:46.539876Z","iopub.status.idle":"2025-12-15T13:50:46.813048Z","shell.execute_reply.started":"2025-12-15T13:50:46.539850Z","shell.execute_reply":"2025-12-15T13:50:46.812452Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define the number of epochs\nnum_epochs = 10\n\n# Train the model\nfor epoch in range(num_epochs):\n\n    # ---- TRAIN ----\n    model.train()\n    train_loss = 0.0\n    train_acc = 0.0\n\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * inputs.size(0)\n\n        # ---- TRAIN ACCURACY ----\n        _, train_preds = torch.max(outputs, 1)\n        train_acc += torch.sum(train_preds == labels.data)\n\n    # ---- EVAL ----\n    model.eval()\n    test_loss = 0.0\n    test_acc = 0.0\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            test_loss += loss.item() * inputs.size(0)\n\n            _, preds = torch.max(outputs, 1)\n            test_acc += torch.sum(preds == labels.data)\n\n    # Normalize metrics\n    train_loss /= len(train_dataset)\n    test_loss /= len(val_dataset)\n\n    train_acc = train_acc.double() / len(train_dataset)\n    test_acc = test_acc.double() / len(val_dataset)\n\n    # ---- PRINT ----\n    print(\n        f\"Epoch [{epoch + 1}/{num_epochs}] \"\n        f\"Train Loss: {train_loss:.4f} \"\n        f\"Train Acc: {train_acc:.4f} \"\n        f\"val Loss: {test_loss:.4f} \"\n        f\"val Acc: {test_acc:.4f}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:50:46.815304Z","iopub.execute_input":"2025-12-15T13:50:46.816017Z","iopub.status.idle":"2025-12-15T13:54:27.720139Z","shell.execute_reply.started":"2025-12-15T13:50:46.815995Z","shell.execute_reply":"2025-12-15T13:54:27.719182Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10] Train Loss: 2.7945 Train Acc: 0.1782 val Loss: 2.3155 val Acc: 0.3765\nEpoch [2/10] Train Loss: 1.8434 Train Acc: 0.5714 val Loss: 1.6349 val Acc: 0.5823\nEpoch [3/10] Train Loss: 1.2214 Train Acc: 0.7775 val Loss: 1.2220 val Acc: 0.6982\nEpoch [4/10] Train Loss: 0.8026 Train Acc: 0.8874 val Loss: 0.9350 val Acc: 0.7866\nEpoch [5/10] Train Loss: 0.5334 Train Acc: 0.9450 val Loss: 0.7484 val Acc: 0.8216\nEpoch [6/10] Train Loss: 0.3526 Train Acc: 0.9782 val Loss: 0.6089 val Acc: 0.8704\nEpoch [7/10] Train Loss: 0.2348 Train Acc: 0.9916 val Loss: 0.5118 val Acc: 0.9085\nEpoch [8/10] Train Loss: 0.1645 Train Acc: 0.9962 val Loss: 0.4416 val Acc: 0.9223\nEpoch [9/10] Train Loss: 0.1182 Train Acc: 0.9985 val Loss: 0.3977 val Acc: 0.9329\nEpoch [10/10] Train Loss: 0.0915 Train Acc: 0.9985 val Loss: 0.3634 val Acc: 0.9314\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns\nimport os\n\nos.makedirs(\"/kaggle/working/metrics\", exist_ok=True)\n\nmodel.eval()\nall_labels = []\nall_preds = []\nall_probs = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        probs = torch.softmax(outputs, dim=1)\n        preds = outputs.argmax(dim=1)\n\n        all_labels.append(labels.cpu().numpy())\n        all_preds.append(preds.cpu().numpy())\n        all_probs.append(probs.cpu().numpy())\n\nall_labels = np.concatenate(all_labels)\nall_preds = np.concatenate(all_preds)\nall_probs = np.concatenate(all_probs)\n\n# ---- Confusion Matrix as PNG ----\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(10,8))\nsns.heatmap(cm, annot=False, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.savefig(\"/kaggle/working/metrics/confusion_matrix.png\")\nplt.close()\n\n# ---- Classification Report as TXT ----\nreport = classification_report(all_labels, all_preds, zero_division=0)\nwith open(\"/kaggle/working/metrics/classification_report.txt\", \"w\") as f:\n    f.write(report)\n\nprint(\"Classification Report:\\n\", report)\n\n# ---- Smart ROC & AUC ----\nMAX_ROC_SAMPLES = 1500  # optional to speed up\n\nif len(all_labels) > MAX_ROC_SAMPLES:\n    idx = np.random.choice(len(all_labels), MAX_ROC_SAMPLES, replace=False)\n    y_true = all_labels[idx]\n    y_prob = all_probs[idx]\nelse:\n    y_true = all_labels\n    y_prob = all_probs\n\nNUM_CLASSES = y_prob.shape[1]\ny_true_bin = label_binarize(y_true, classes=list(range(NUM_CLASSES)))\n\n# Micro-average ROC\nfpr_micro, tpr_micro, _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\nroc_auc_micro = auc(fpr_micro, tpr_micro)\n\n# Macro-average ROC\nfpr_macro = np.linspace(0, 1, 100)\ntpr_macro = np.zeros_like(fpr_macro)\nfor i in range(NUM_CLASSES):\n    fpr_i, tpr_i, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n    tpr_macro += np.interp(fpr_macro, fpr_i, tpr_i)\ntpr_macro /= NUM_CLASSES\nroc_auc_macro = auc(fpr_macro, tpr_macro)\n\n# Save ROC AUC in TXT\nwith open(\"/kaggle/working/metrics/roc_auc.txt\", \"w\") as f:\n    f.write(f\"Micro-average AUC: {roc_auc_micro:.4f}\\n\")\n    f.write(f\"Macro-average AUC: {roc_auc_macro:.4f}\\n\")\n\n# Optional: Save ROC curves as image\nplt.figure(figsize=(8,6))\nplt.plot(fpr_micro, tpr_micro,\n         label=f'Micro-average ROC (AUC = {roc_auc_micro:.4f})', linewidth=2)\nplt.plot(fpr_macro, tpr_macro,\n         label=f'Macro-average ROC (AUC = {roc_auc_macro:.4f})', linewidth=2)\nplt.plot([0,1], [0,1], 'k--', linewidth=1)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves')\nplt.legend(loc='lower right')\nplt.savefig(\"/kaggle/working/metrics/roc_curves.png\")\nplt.close()\n\nprint(f\"Micro AUC: {roc_auc_micro:.4f}, Macro AUC: {roc_auc_macro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:54:27.721078Z","iopub.execute_input":"2025-12-15T13:54:27.721435Z","iopub.status.idle":"2025-12-15T13:54:44.902593Z","shell.execute_reply.started":"2025-12-15T13:54:27.721414Z","shell.execute_reply":"2025-12-15T13:54:44.901725Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       0.83      0.89      0.86        44\n           2       0.53      0.53      0.53        32\n           3       0.59      0.70      0.64        43\n           4       0.76      0.76      0.76        42\n           5       0.71      0.68      0.69        40\n           6       0.94      0.74      0.83        39\n           7       0.70      0.51      0.59        45\n           8       0.66      0.56      0.61        41\n           9       0.66      0.76      0.70        33\n          10       0.87      0.89      0.88        38\n          11       0.80      0.80      0.80        40\n          12       0.77      0.71      0.74        42\n          13       0.61      0.68      0.64        41\n          14       0.71      0.84      0.77        43\n          15       0.62      0.78      0.69        36\n          16       0.55      0.51      0.53        45\n          17       0.38      0.41      0.40        39\n          18       0.72      0.50      0.59        42\n          19       0.45      0.50      0.47        42\n\n    accuracy                           0.69       811\n   macro avg       0.69      0.69      0.69       811\nweighted avg       0.69      0.69      0.69       811\n\nMicro AUC: 0.9729, Macro AUC: 0.9690\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}